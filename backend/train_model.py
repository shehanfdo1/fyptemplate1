# backend/train_model.py
import pandas as pd
import re
import joblib
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from datasets import load_dataset

# 1. Load Dataset (Using Hugging Face to ensure it works on your Mac)
print("üì• Loading dataset...")
dataset = load_dataset("zefang-liu/phishing-email-dataset")
df = dataset['train'].to_pandas()

print("Original Columns:", df.columns.tolist())

# 2. Preprocessing
# Standardize column names based on your prototype
df = df.rename(columns={"Email Text": "text", "Email Type": "label"})
df = df.dropna(subset=['text', 'label'])

# --- CUSTOM DATA INJECTION ---
try:
    from custom_data import custom_emails
    print(f"üíâ Injecting {len(custom_emails)} custom examples...")
    custom_df = pd.DataFrame(custom_emails)
    # Rename columns to match the main dataframe if needed. 
    # The main dataframe uses 'text' and 'label' now after renaming.
    # Our custom_data.py already uses 'text' and 'label'.
    
    # NEW: Oversample custom data to ensure model learns it
    custom_df = pd.concat([custom_df] * 100, ignore_index=True)
    
    df = pd.concat([df, custom_df], ignore_index=True)
    print(f"üìä New dataset size: {len(df)}")
except ImportError:
    print("‚ö†Ô∏è Warning: custom_data.py not found. skipping injection.")
except Exception as e:
    print(f"‚ö†Ô∏è Error injecting custom data: {e}")
# -----------------------------

def clean_email_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+', ' ', text)  # remove URLs
    text = re.sub(r'\S+@\S+', ' ', text)  # remove emails
    text = re.sub(r'[^a-z\s]', ' ', text)  # remove non-alphabetic
    text = re.sub(r'\s+', ' ', text)      # remove extra spaces
    return text.strip()

print("üßπ Cleaning text...")
df['clean_text'] = df['text'].apply(clean_email_text)

# Map labels: Safe=0, Phishing=1 (CORRECTED MAPPING)
# Keys must exactly match the strings in the 'Email Type'/'label' column.
df['label_num'] = df['label'].map({
    'Phishing Email': 1,  # Phishing Class
    'Safe Email': 0       # Safe/Legitimate Class
    # These are the two primary labels in this dataset.
})
print("Unique Mapped Labels BEFORE Drop:", df['label_num'].unique())

# Drop rows where label_num could not be mapped (NaN)
df = df.dropna(subset=['label_num'])

print("Final Class Counts:")
print(df['label_num'].value_counts())
# This must show counts for both 0.0 and 1.0 for training to succeed.

# 3. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['label_num'], test_size=0.2, random_state=42, stratify=df['label_num']
)

# 4. TF-IDF Vectorization (Improved parameters)
# min_df=2 allows words that appear in at least 2 emails (better for small custom datasets)
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', min_df=2, ngram_range=(1,2))
X_train_vec = vectorizer.fit_transform(X_train)

# 5. Logistic Regression (Added Class Weighting)
# class_weight='balanced' helps if you have more phishing than safe emails (or vice versa)
model = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000)
model.fit(X_train_vec, y_train)

# 6. Save the new files
joblib.dump(model, "email_phishing_model.pkl")
joblib.dump(vectorizer, "email_vectorizer.pkl")

print(f"‚úÖ Training Complete! Accuracy: {model.score(vectorizer.transform(X_test), y_test):.2%}")
print("üìÇ Files 'email_phishing_model.pkl' and 'email_vectorizer.pkl' have been updated.")